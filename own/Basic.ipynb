{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import vizdoom as vzd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from vizdoom import GameVariable\n",
    "from time import sleep\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets time that will pause the engine after each action (in seconds)\n",
    "# Without this everything would go too fast for you to keep track of what's happening.\n",
    "sleep_time = 1.0 / vzd.DEFAULT_TICRATE  # = 0.028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_game():\n",
    "    # Create DoomGame instance. It will run the game and communicate with you.\n",
    "    game = vzd.DoomGame()\n",
    "\n",
    "    # Now it's time for configuration!\n",
    "    # load_config could be used to load configuration instead of doing it here with code.\n",
    "    # If load_config is used in-code configuration will also work - most recent changes will add to previous ones.\n",
    "    # game.load_config(\"my_basic.cfg\") TODO\n",
    "\n",
    "    # Sets path to additional resources wad file which is basically your scenario wad.\n",
    "    # If not specified default maps will be used and it's pretty much useless... unless you want to play good old Doom.\n",
    "    game.set_doom_scenario_path(\"../scenarios/basic.wad\")\n",
    "\n",
    "    # Sets map to start (scenario .wad files can contain many maps).\n",
    "    game.set_doom_map(\"map01\")\n",
    "\n",
    "    # Sets resolution. Default is 320X240\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "\n",
    "    # Sets the screen buffer format. Not used here but now you can change it. Default is CRCGCB.\n",
    "    game.set_screen_format(vzd.ScreenFormat.RGB24)\n",
    "\n",
    "    # Enables depth buffer.\n",
    "    game.set_depth_buffer_enabled(True)\n",
    "\n",
    "    # Enables labeling of in game objects labeling.\n",
    "    game.set_labels_buffer_enabled(True)\n",
    "\n",
    "    # Enables buffer with top down map of the current episode/level.\n",
    "    game.set_automap_buffer_enabled(True)\n",
    "\n",
    "    # Enables information about all objects present in the current episode/level.\n",
    "    game.set_objects_info_enabled(True)\n",
    "\n",
    "    # Enables information about all sectors (map layout).\n",
    "    game.set_sectors_info_enabled(True)\n",
    "\n",
    "    # Sets other rendering options (all of these options except crosshair are enabled (set to True) by default)\n",
    "    game.set_render_hud(False)\n",
    "    game.set_render_minimal_hud(False)  # If hud is enabled\n",
    "    game.set_render_crosshair(True)\n",
    "    game.set_render_weapon(True)\n",
    "    game.set_render_decals(False)  # Bullet holes and blood on the walls\n",
    "    game.set_render_particles(False)\n",
    "    game.set_render_effects_sprites(False)  # Smoke and blood\n",
    "    game.set_render_messages(False)  # In-game messages\n",
    "    game.set_render_corpses(False)\n",
    "    game.set_render_screen_flashes(True)  # Effect upon taking damage or picking up items\n",
    "\n",
    "    # Adds buttons that will be allowed.\n",
    "    game.add_available_button(vzd.Button.MOVE_LEFT)\n",
    "    game.add_available_button(vzd.Button.MOVE_RIGHT)\n",
    "    game.add_available_button(vzd.Button.ATTACK)\n",
    "\n",
    "    # Adds game variables that will be included in state.\n",
    "    game.add_available_game_variable(vzd.GameVariable.AMMO2)\n",
    "\n",
    "    # Causes episodes to finish after 200 tics (actions)\n",
    "    game.set_episode_timeout(200)\n",
    "\n",
    "    # Makes episodes start after 10 tics (~after raising the weapon)\n",
    "    game.set_episode_start_time(10)\n",
    "\n",
    "    # Makes the window appear (turned on by default)\n",
    "    game.set_window_visible(True)\n",
    "\n",
    "    # Turns on the sound. (turned off by default)\n",
    "    game.set_sound_enabled(False)\n",
    "\n",
    "    # Sets the living reward (for each move) to -1\n",
    "    game.set_living_reward(-1)\n",
    "\n",
    "    # Sets ViZDoom mode (PLAYER, ASYNC_PLAYER, SPECTATOR, ASYNC_SPECTATOR, PLAYER mode is default)\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "\n",
    "    # Define some actions. Each list entry corresponds to declared buttons:\n",
    "    # MOVE_LEFT, MOVE_RIGHT, ATTACK\n",
    "    # game.get_available_buttons_size() can be used to check the number of available buttons.\n",
    "    # 5 more combinations are naturally possible but only 3 are included for transparency when watching.\n",
    "    actions = [[True, False, False], [False, True, False], [False, False, True]]\n",
    "    \n",
    "    return game, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(game, agent, actions, episodes, verbose=True, print_step_info=False):\n",
    "    game.init()\n",
    "\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        global_step = 0\n",
    "        done = False\n",
    "        print(\"Episode #\" + str(i + 1))\n",
    "        \n",
    "        stack_size = 4\n",
    "        stacked_frames = deque([torch.zeros((299 , 399)) for i in range(stack_size)], maxlen=stack_size)\n",
    "        observation = preprocess_stacked_frames(stacked_frames)\n",
    "        # fill the initial deque with zeros of the same shape the frame is after preprocessing\n",
    "\n",
    "        while not game.is_episode_finished():\n",
    "            old_observation = observation\n",
    "            state = game.get_state()\n",
    "            frame = preprocess_frame(state.screen_buffer)\n",
    "            stacked_frames.append(frame)\n",
    "                \n",
    "            observation = preprocess_stacked_frames(stacked_frames)\n",
    "            action = agent.get_action(observation)\n",
    "            reward = game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            agent.append_memory(old_observation, torch.tensor(action),\n",
    "                                torch.tensor(reward), observation, torch.tensor(done))\n",
    "            \n",
    "            if global_step > agent.batch_size:\n",
    "                agent.train()\n",
    "            \n",
    "            if print_step_info:\n",
    "                print(\"State #\" + str(state.number))\n",
    "                print(\"Reward:\", reward)\n",
    "                print(\"=====================\")\n",
    "\n",
    "            if sleep_time > 0:\n",
    "                sleep(sleep_time)\n",
    "            global_step += 1\n",
    "        if verbose:        \n",
    "            print(\"Episode finished.\")\n",
    "            print(\"Total reward:\", game.get_total_reward())\n",
    "            print(\"************************\")\n",
    "\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(img):\n",
    "    rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "    img = img @ rgb_weights\n",
    "    img = img[181:,121:-120]\n",
    "    \n",
    "    return torch.tensor(img).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stacked_frames(stacked_frames):\n",
    "    stack = torch.stack(tuple(stacked_frames))\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state(state, gray):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    if gray:\n",
    "        plt.imshow(preprocess(screen), \"gray\");\n",
    "    else:\n",
    "        plt.imshow(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(x, action_size):\n",
    "    return torch.eye(action_size)[x].squeeze().bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, 3, padding=0, stride=2, bias=False), # (16, 149, 199)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, padding=(1,0), stride=2, bias=False), # (32, 75, 99)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=0, stride=2, bias=False), # (64, 37, 49)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=0, stride=2, bias=False), # (128, 18, 24)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(128, 100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(100, self.action_size),\n",
    "            nn.Softmax(0)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.mean((2, 3)) # global average pool\n",
    "        x = x.view(-1, 128)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, action_size, actions, epsilon=0.99, memory_size=1000, \n",
    "                 batch_size=64, discount_factor=0.99, lr=1e-4, epsilon_decay=0.99): # TOdo use eps decay\n",
    "        self.action_size = action_size\n",
    "        self.q_net = Qnet(action_size)\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "        self.batch_size = batch_size\n",
    "        self.discount = discount_factor\n",
    "        self.lr = lr\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.opt = optim.Adam(self.q_net.parameters(), lr=self.lr)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            action = torch.argmax(self.q_net(state.unsqueeze(0))).item()\n",
    "            return self.actions[action]\n",
    "        \n",
    "    def append_memory(self, state, action, reward, next_state, done):\n",
    "        # state is the last 4 frames stacked here\n",
    "        # make multiple memory for performance optimization\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train(self):\n",
    "        self.opt.zero_grad()\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        \n",
    "        for sample in batch:\n",
    "            states.append(sample[0])\n",
    "            actions.append(sample[1])\n",
    "            rewards.append(sample[2])\n",
    "            next_states.append(sample[3])\n",
    "            dones.append(sample[4])\n",
    "        \n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions).bool()\n",
    "        actions = to_categorical(actions, self.action_size)\n",
    "        rewards = torch.stack(rewards)\n",
    "        next_states = torch.stack(next_states)\n",
    "        dones = torch.stack(dones).bool()\n",
    "        not_dones = ~dones\n",
    "        \n",
    "        state_values = self.q_net(states)[actions]\n",
    "        next_state_values = torch.max(self.q_net(next_states), 0)\n",
    "        next_state_values = next_state_values.values[not_dones.squeeze()]\n",
    "        \n",
    "        Y = rewards.detach().clone() # is detaching correct?\n",
    "        Y[not_dones] += self.discount * next_state_values\n",
    "        \n",
    "        loss = torch.sum((Y - state_values) ** 2) / self.batch_size\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_game' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4b18fe761124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_game' is not defined"
     ]
    }
   ],
   "source": [
    "game, actions = create_game()\n",
    "agent = DQNAgent(3, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [64, 3] at index 0does not match the shape of the indexed tensor [3, 3] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-1269d63a0b0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_step_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ba70493649cb>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(game, agent, actions, episodes, verbose, print_step_info)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprint_step_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-c00701b7c56c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m#print(actions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-3afa0bb7a335>\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(x, action_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [64, 3] at index 0does not match the shape of the indexed tensor [3, 3] at index 0"
     ]
    }
   ],
   "source": [
    "run(game, agent, actions, 10, print_step_info=False)\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = preprocess_stacked_frames(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = Qnet(action_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = deque([(frames, torch.randn(1), torch.randn(1), frames, torch.randn(1)) for _ in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = random.sample(mem, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "st, ac, r, nst, d = [], [], [], [], []\n",
    "for x in batch:\n",
    "    st.append(x[0])\n",
    "    ac.append(x[1])\n",
    "    r.append(x[2])\n",
    "    nst.append(x[3])\n",
    "    d.append(x[4])\n",
    "    \n",
    "st = torch.stack(st)\n",
    "ac = torch.stack(ac).long()\n",
    "r = torch.stack(r)\n",
    "nst = torch.stack(nst)\n",
    "d = torch.stack(d).bool()\n",
    "d[1] = False\n",
    "\n",
    "Y = r.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4415],\n",
       "        [-0.6384],\n",
       "        [ 0.3228]])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = ~d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3333], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(q(nst), 0).values[nd.squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[nd] += 0.99 * torch.max(q(nst), 0).values[nd.squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4415],\n",
       "        [-0.3084],\n",
       "        [ 0.3228]], grad_fn=<IndexPutBackward>)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6510, -0.8645, -0.5376])"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo[to_categorical(ac, 3)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
