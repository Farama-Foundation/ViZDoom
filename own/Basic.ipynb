{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import vizdoom as vzd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from vizdoom import GameVariable\n",
    "from time import sleep\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets time that will pause the engine after each action (in seconds)\n",
    "# Without this everything would go too fast for you to keep track of what's happening.\n",
    "sleep_time = 1.0 / vzd.DEFAULT_TICRATE  # = 0.028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_game():\n",
    "    # Create DoomGame instance. It will run the game and communicate with you.\n",
    "    game = vzd.DoomGame()\n",
    "\n",
    "    # Now it's time for configuration!\n",
    "    # load_config could be used to load configuration instead of doing it here with code.\n",
    "    # If load_config is used in-code configuration will also work - most recent changes will add to previous ones.\n",
    "    # game.load_config(\"my_basic.cfg\") TODO\n",
    "\n",
    "    # Sets path to additional resources wad file which is basically your scenario wad.\n",
    "    # If not specified default maps will be used and it's pretty much useless... unless you want to play good old Doom.\n",
    "    game.set_doom_scenario_path(\"../scenarios/basic.wad\")\n",
    "\n",
    "    # Sets map to start (scenario .wad files can contain many maps).\n",
    "    game.set_doom_map(\"map01\")\n",
    "\n",
    "    # Sets resolution. Default is 320X240\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "\n",
    "    # Sets the screen buffer format. Not used here but now you can change it. Default is CRCGCB.\n",
    "    game.set_screen_format(vzd.ScreenFormat.RGB24)\n",
    "\n",
    "    # Enables depth buffer.\n",
    "    game.set_depth_buffer_enabled(True)\n",
    "\n",
    "    # Enables labeling of in game objects labeling.\n",
    "    game.set_labels_buffer_enabled(True)\n",
    "\n",
    "    # Enables buffer with top down map of the current episode/level.\n",
    "    game.set_automap_buffer_enabled(True)\n",
    "\n",
    "    # Enables information about all objects present in the current episode/level.\n",
    "    game.set_objects_info_enabled(True)\n",
    "\n",
    "    # Enables information about all sectors (map layout).\n",
    "    game.set_sectors_info_enabled(True)\n",
    "\n",
    "    # Sets other rendering options (all of these options except crosshair are enabled (set to True) by default)\n",
    "    game.set_render_hud(False)\n",
    "    game.set_render_minimal_hud(False)  # If hud is enabled\n",
    "    game.set_render_crosshair(True)\n",
    "    game.set_render_weapon(True)\n",
    "    game.set_render_decals(False)  # Bullet holes and blood on the walls\n",
    "    game.set_render_particles(False)\n",
    "    game.set_render_effects_sprites(False)  # Smoke and blood\n",
    "    game.set_render_messages(False)  # In-game messages\n",
    "    game.set_render_corpses(False)\n",
    "    game.set_render_screen_flashes(True)  # Effect upon taking damage or picking up items\n",
    "\n",
    "    # Adds buttons that will be allowed.\n",
    "    game.add_available_button(vzd.Button.MOVE_LEFT)\n",
    "    game.add_available_button(vzd.Button.MOVE_RIGHT)\n",
    "    game.add_available_button(vzd.Button.ATTACK)\n",
    "\n",
    "    # Adds game variables that will be included in state.\n",
    "    game.add_available_game_variable(vzd.GameVariable.AMMO2)\n",
    "\n",
    "    # Causes episodes to finish after 200 tics (actions)\n",
    "    game.set_episode_timeout(200)\n",
    "\n",
    "    # Makes episodes start after 10 tics (~after raising the weapon)\n",
    "    game.set_episode_start_time(10)\n",
    "\n",
    "    # Makes the window appear (turned on by default)\n",
    "    game.set_window_visible(True)\n",
    "\n",
    "    # Turns on the sound. (turned off by default)\n",
    "    game.set_sound_enabled(False)\n",
    "\n",
    "    # Sets the living reward (for each move) to -1\n",
    "    game.set_living_reward(-1)\n",
    "\n",
    "    # Sets ViZDoom mode (PLAYER, ASYNC_PLAYER, SPECTATOR, ASYNC_SPECTATOR, PLAYER mode is default)\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "\n",
    "    # Define some actions. Each list entry corresponds to declared buttons:\n",
    "    # MOVE_LEFT, MOVE_RIGHT, ATTACK\n",
    "    # game.get_available_buttons_size() can be used to check the number of available buttons.\n",
    "    # 5 more combinations are naturally possible but only 3 are included for transparency when watching.\n",
    "    actions = [[True, False, False], [False, True, False], [False, False, True]]\n",
    "    \n",
    "    return game, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(game, agent, actions, episodes, verbose=True, print_step_info=False):\n",
    "    game.init()\n",
    "\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        global_step = 0\n",
    "        done = False\n",
    "        print(\"Episode #\" + str(i + 1))\n",
    "        \n",
    "        stack_size = 4\n",
    "        stacked_frames = deque([torch.zeros((299 , 399)) for i in range(stack_size)], maxlen=stack_size)\n",
    "        observation = preprocess_stacked_frames(stacked_frames)\n",
    "        # fill the initial deque with zeros of the same shape the frame is after preprocessing\n",
    "\n",
    "        while not game.is_episode_finished():\n",
    "            old_observation = observation\n",
    "            state = game.get_state()\n",
    "            frame = preprocess_frame(state.screen_buffer)\n",
    "            stacked_frames.append(frame)\n",
    "                \n",
    "            observation = preprocess_stacked_frames(stacked_frames)\n",
    "            action = agent.get_action(observation)\n",
    "            reward = game.make_action(actions[action])\n",
    "            done = game.is_episode_finished()\n",
    "            agent.append_memory(old_observation, torch.tensor(action),\n",
    "                                torch.tensor(reward), observation, torch.tensor(done))\n",
    "            \n",
    "            \n",
    "            if global_step > agent.batch_size:\n",
    "                agent.train()\n",
    "            \n",
    "            #if done:\n",
    "            #    return observation\n",
    "            \n",
    "            if print_step_info:\n",
    "                print(\"State #\" + str(state.number))\n",
    "                print(\"Reward:\", reward)\n",
    "                print(\"=====================\")\n",
    "\n",
    "            if sleep_time > 0:\n",
    "                sleep(sleep_time)\n",
    "            global_step += 1\n",
    "        if verbose:        \n",
    "            print(\"Episode finished.\")\n",
    "            print(\"Total reward:\", game.get_total_reward())\n",
    "            print(\"************************\")\n",
    "\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(img):\n",
    "    rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "    img = img @ rgb_weights\n",
    "    img = img[181:,121:-120]\n",
    "    \n",
    "    return torch.tensor(img).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stacked_frames(stacked_frames):\n",
    "    stack = torch.stack(tuple(stacked_frames))\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state(state, gray):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    if gray:\n",
    "        plt.imshow(preprocess(screen), \"gray\");\n",
    "    else:\n",
    "        plt.imshow(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(x, action_size):\n",
    "    return torch.eye(action_size)[x].squeeze().bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, 3, padding=0, stride=2, bias=False), # (16, 149, 199)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, padding=(1,0), stride=2, bias=False), # (32, 75, 99)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=0, stride=2, bias=False), # (64, 37, 49)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=0, stride=2, bias=False), # (128, 18, 24)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(128, 100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(100, self.action_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.mean((2, 3)) # global average pool\n",
    "        x = x.view(-1, 128)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, action_size, actions, epsilon=0.99, memory_size=1000, \n",
    "                 batch_size=8, discount_factor=0.99, lr=1e-4, epsilon_decay=0.99): # TOdo use eps decay\n",
    "        self.action_size = action_size\n",
    "        self.q_net = Qnet(action_size)\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "        self.batch_size = batch_size\n",
    "        self.discount = discount_factor\n",
    "        self.lr = lr\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.opt = optim.Adam(self.q_net.parameters(), lr=self.lr)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return random.choice(range(self.action_size))\n",
    "        else:\n",
    "            action = torch.argmax(self.q_net(state.unsqueeze(0))).item()\n",
    "            return action\n",
    "        \n",
    "    def append_memory(self, state, action, reward, next_state, done):\n",
    "        # state is the last 4 frames stacked here\n",
    "        # make multiple memory for performance optimization\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train(self):\n",
    "        self.opt.zero_grad()\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        \n",
    "        for sample in batch:\n",
    "            states.append(sample[0])\n",
    "            actions.append(sample[1])\n",
    "            rewards.append(sample[2])\n",
    "            next_states.append(sample[3])\n",
    "            dones.append(sample[4])\n",
    "        \n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions).long()\n",
    "        #actions = to_categorical(actions, self.action_size)\n",
    "        rewards = torch.stack(rewards)\n",
    "        next_states = torch.stack(next_states)\n",
    "        dones = torch.stack(dones).bool()\n",
    "        not_dones = ~dones\n",
    "        \n",
    "        state_values = self.q_net(states)[to_categorical(actions, 3)]\n",
    "        next_state_values = torch.max(self.q_net(next_states), 1)\n",
    "        next_state_values = next_state_values.values[not_dones.squeeze()]\n",
    "        \n",
    "        Y = rewards.detach().clone() # is detaching correct?\n",
    "        Y[not_dones] += self.discount * next_state_values\n",
    "        \n",
    "        loss = torch.sum((Y - state_values) ** 2) / self.batch_size\n",
    "        loss.backward()\n",
    "        self.opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()\n",
    "game, actions = create_game()\n",
    "agent = DQNAgent(3, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #1\n",
      "State #1\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #2\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #3\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #4\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #5\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #6\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #7\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #8\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #9\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #10\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #11\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #12\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #13\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #14\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #15\n",
      "Reward: 100.0\n",
      "=====================\n",
      "State #16\n",
      "Reward: -1.0\n",
      "=====================\n",
      "Episode finished.\n",
      "Total reward: 85.0\n",
      "************************\n",
      "Episode #2\n",
      "State #1\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #2\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #3\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #4\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #5\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #6\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #7\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #8\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #9\n",
      "Reward: -6.0\n",
      "=====================\n",
      "State #10\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #11\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #12\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #13\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #14\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #15\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #16\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #17\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #18\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #19\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #20\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #21\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #22\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #23\n",
      "Reward: -6.0\n",
      "=====================\n",
      "State #24\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #25\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #26\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #27\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #28\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #29\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #30\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #31\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #32\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #33\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #34\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #35\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #36\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #37\n",
      "Reward: -6.0\n",
      "=====================\n",
      "State #38\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #39\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #40\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #41\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #42\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #43\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #44\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #45\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #46\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #47\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #48\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #49\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #50\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #51\n",
      "Reward: -6.0\n",
      "=====================\n",
      "State #52\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #53\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #54\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #55\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #56\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #57\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #58\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #59\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #60\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #61\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #62\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #63\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #64\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #65\n",
      "Reward: -6.0\n",
      "=====================\n",
      "State #66\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #67\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #68\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #69\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #70\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #71\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #72\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #73\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #74\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #75\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #76\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #77\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #78\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #79\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #80\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #81\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #82\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #83\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #84\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #85\n",
      "Reward: -6.0\n",
      "=====================\n",
      "State #86\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #87\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #88\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #89\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #90\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #91\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #92\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #93\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #94\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #95\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #96\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #97\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #98\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #99\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #100\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #101\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #102\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #103\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #104\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #105\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #106\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #107\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #108\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #109\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #110\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #111\n",
      "Reward: -6.0\n",
      "=====================\n",
      "State #112\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #113\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #114\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #115\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #116\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #117\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #118\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #119\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #120\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #121\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #122\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #123\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #124\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #125\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #126\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #127\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #128\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #129\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #130\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #131\n",
      "Reward: -6.0\n",
      "=====================\n",
      "State #132\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #133\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #134\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #135\n",
      "Reward: -1.0\n",
      "=====================\n",
      "State #136\n",
      "Reward: -1.0\n",
      "=====================\n"
     ]
    },
    {
     "ename": "SignalException",
     "evalue": "Signal SIGINT received. ViZDoom instance has been closed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSignalException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-9d1c62e5b6f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_step_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5ba13de68db3>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(game, agent, actions, episodes, verbose, print_step_info)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_stacked_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_episode_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             agent.append_memory(old_observation, torch.tensor(action),\n",
      "\u001b[0;31mSignalException\u001b[0m: Signal SIGINT received. ViZDoom instance has been closed."
     ]
    }
   ],
   "source": [
    "run(game, agent, actions, 10, print_step_info=True)\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = Qnet(action_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = deque([(obs, torch.randn(1), torch.randn(1), obs, torch.randn(1)) for _ in range(64)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = random.sample(mem, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st, ac, r, nst, d = [], [], [], [], []\n",
    "for x in batch:\n",
    "    st.append(x[0])\n",
    "    ac.append(x[1])\n",
    "    r.append(x[2])\n",
    "    nst.append(x[3])\n",
    "    d.append(x[4])\n",
    "    \n",
    "st = torch.stack(st)\n",
    "ac = torch.stack(ac).long()\n",
    "r = torch.stack(r)\n",
    "nst = torch.stack(nst)\n",
    "d = torch.stack(d).bool()\n",
    "d[1] = False\n",
    "\n",
    "Y = r.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = ~d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q(nst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(q(nst), 1).values[nd.squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[nd] += 0.99 * torch.max(q(nst), 1).values[nd.squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo[to_categorical(ac, 3)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
